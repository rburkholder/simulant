2015/07/11

multi-paradigm control
1) use source media, make into a set of clips like adobe premiere
     fit the clips into a timeline for playing on fixed pre-programmed sets
2) free flow:  create cues which are based seek, play, stop, loop, something like show control system
3) then mix and max combinations of the two

* key is to be able to pull out time stamps, view waveforms and media files, and coordinate the efforts
* will require various views on the data, additional serialization tasks

* media file list
* clip list
* timelines
* cue lists based upon events

2015/06/01
libraries:
libavdevice, libavffilter, libavformat, libavcodec, libavresample, libavutil, libswscale
libz, pthread, png, boost(system, thread, chrono, filesystem, serialization),
GL, wt_gtk3u_gl-3.1, GLU, SOIL, bz2, rtaudio


2015/05/23
apt-get install librtaudio-dev


2015/05/03

https://libav.org/releases/libav-11.3.release
One specific API issue in libavformat deserves mentioning here. When using
libavcodec for decoding or encoding and libavformat for demuxing or muxing,
the standard practice was to use the stream codec context (AVStream.codec) for
actual decoding or encoding. There are multiple problems with this pattern
(the main one is that the decoder/demuxer or encoder/muxer are not necessarily
synchronized and may overwrite each other's state), so it is now strongly
discouraged and will likely be deprecated in the future. Users should instead
allocate a separate decoding or encoding context and populate it from the
demuxing codec context (or the reverse for encoding) with the
avcodec_copy_context() function.

2015/03/22

Apparently, wxWidgets requires that the first OpenGL or GLEW call be placed inside the OnPaint event handler function. 
Look for EVT_PAINT to find this function. Once I did that, the error disappeared and OpenGL worked.
http://choorucode.com/2014/07/22/segmentation-fault-at-glcreateshader-in-wxwidgets/

2015/03/15

opengl tutorial files:  https://bitbucket.org/alfonse/gltut/downloads
main site (currently offline): http://arcsynthesis.org

2015/03/xx

h.264 test media:  http://dash-mse-test.appspot.com/decoder-test.html

better:
sample working h.264 file at:
https://github.com/raspberrypi/userland/blob/master/host_applications/linux/apps/hello_pi/hello_video/test.h264

need more flexible decoding for other files.



tinylib: https://github.com/roxlu/tinylib.git
example h.264 player with tinylib and libav:  http://roxlu.com/2014/039/decoding-h264-and-yuv420p-playback

opengl tutorial:  http://www.arcsynthesis.org/gltut/index.html

=====
libav build:
git clone git://git.libav.org/libav.git
apt-get install nasm
mkdir buildNormal
#../configure --enable-libfaac --enable-libfdk-aac --enable-libopenh264 --enable-libopus --enable-libvpx --enable-nonfree
../configure --host-cppflags=-Ofast
make
make install


=====

wxWidgets

mkdir buildNormal
cd buildNormal
../configure --enable-threads --with-gtk=3 --enable-stl  --with-opengl    CXXFLAGS=-Ofast

=====
 2015/03/15

need to run the pipeline so that decode, scale, transform are computed
then special stuff can be performed on final perspective
scaling may not be needed if performed within opengl.
looks like opengl is our next stop to handle final transformations and placements within the view